{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MLOps with datalab\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 What is MLOps?\n",
    "\n",
    "**MLOps** stands for `Machine Learning Operations`. It contains a set of best practices that seeks to increase automation and improve the efficiency of models development and deployment.\n",
    "\n",
    "### 1.2 Why do we need MLOps? Git is not enough?\n",
    "\n",
    "Put a machine learning model into production is difficult. It envoles many complex aspects such as\n",
    "- data collection/ingest,\n",
    "- data prep (e.g. cleaning, feature engineering, etc),\n",
    "- model development\n",
    "- model training,\n",
    "- model tuning\n",
    "- model deployment\n",
    "- model monitoring,\n",
    "- model explainability\n",
    "- ETC.\n",
    "\n",
    "Below figure shows the different aspects of mlops :\n",
    "\n",
    "![ml_technical_debt.PNG](img/ml_technical_debt.PNG)\n",
    "\n",
    "### 1.3 ML Operations\n",
    "\n",
    "We need to address the following MLOps principals:\n",
    "\n",
    "- **Model tracking**: track all the necessary element to reproduce the model such as code, hyperparameter and training data.\n",
    "- **Model review**: Test model and produce quality assurance report. Inference model production-specifics properties such as model response times.\n",
    "- **Model Governance** : manage model versions, model artifacts and transitions through their lifecycle (e.g. staging, production, archived,etc.).\n",
    "\n",
    "- **Model deployment**: Automate the process of deploying registered models (e.g. permissions, cluster creation, API management, etc.)\n",
    "- **Model monitoring**: Monitor the state of model production server (e.g. number of request, response time, serving data anomalies, etc.)\n",
    "- **Model retraining**: Create alerts and automation to take corrective action in case of **model drift** due to\n",
    "                    differences in training and inference data or `data evolution`.\n",
    "\n",
    "\n",
    "### 1.4 Continuous X\n",
    "\n",
    "- **CI**: Track model code, training data( e.g. Feature engineering/selection), hyper-parameters optimization\n",
    "- **CD**: Need to deliver not only an executable package, but also a complete pipeline of how the model is trained.\n",
    "- **CT(Continuous training)**: Models need to be retrained automatically. Because evolving data make your model decay. data validation is essential at this step, Because data drifting can be caused by evolution or errors.\n",
    "\n",
    "## 2 Illustrate mlops via a application example\n",
    "\n",
    "## 2.1 The context\n",
    "\n",
    "If you are a pokemon go player, when you capture a new pokemon, you may want to know if this pokemon is good or not. To make this easier, we would like to train a classification model that can tell us if the pokemon is legendary or not.\n",
    "\n",
    "In this tutorial we will use a random forest classifier to implement this model\n",
    "\n",
    "## 2.2 Prepare environment and install the dependencies\n",
    "\n",
    "Launch a jupyter service in datalab.\n",
    "\n",
    "**Don't forget to assign admin role in kubernetes tab**\n",
    "\n",
    "![jupyter_datalab.PNG](img/jupyter_datalab.PNG)\n",
    "\n",
    "Start a terminal in jupyter\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/pengfei99/MLOPS.git\n",
    "```\n",
    "\n",
    "Then install the dependencies\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 1. Train a model in an old school way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate an accuracy from the confusion matrix\n",
    "def get_model_accuracy(cf_matrix):\n",
    "    diagonal_sum = cf_matrix.trace()\n",
    "    sum_of_all_elements = cf_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements\n",
    "\n",
    "\n",
    "def train_model(data_url: str, n_estimator: int, max_depth: int, min_samples_split: int):\n",
    "    print(f\"data source: {data_url}\")\n",
    "    feature_data, label_data = prepare_data(data_url)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(feature_data, label_data, train_size=0.8, test_size=0.2,\n",
    "                                                        random_state=0)\n",
    "    # print(len(test_X))\n",
    "\n",
    "    # create a random forest classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth,\n",
    "                                    min_samples_split=min_samples_split,\n",
    "                                    n_jobs=2, random_state=0)\n",
    "    # train the model with training_data\n",
    "    rf_clf.fit(train_X, train_y)\n",
    "    # predict testing data\n",
    "    predicts_val = rf_clf.predict(test_X)\n",
    "\n",
    "    # Generate a cm\n",
    "    cm = confusion_matrix(test_y, predicts_val)\n",
    "    model_accuracy = get_model_accuracy(cm)\n",
    "    print(\"RandomForest model with hyper-parameters: (n_estimator=%f, max_depth=%f, min_samples_split=%f):\" % (\n",
    "        n_estimator, max_depth, min_samples_split))\n",
    "    print(\"accuracy: %f\" % model_accuracy)\n",
    "\n",
    "\n",
    "def prepare_data(data_url):\n",
    "    # read data as df\n",
    "    try:\n",
    "        input_df = pd.read_csv(data_url, index_col=0)\n",
    "        input_df.head()\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Unable to read data from the giving path, check your data location. Error: %s\", e\n",
    "        )\n",
    "    # Prepare data for ml model\n",
    "    label = input_df.legendary\n",
    "    feature = input_df.drop(['legendary', 'generation', 'total'], axis=1).select_dtypes(exclude=['object'])\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data source: https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\n",
      "RandomForest model with hyper-parameters: (n_estimator=50.000000, max_depth=30.000000, min_samples_split=2.000000):\n",
      "accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "# set the training data path\n",
    "data_url = \"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "\n",
    "# set the hyper parameters\n",
    "n_estimator = 50\n",
    "max_depth = 30\n",
    "min_samples_split = 2\n",
    "\n",
    "# train the model\n",
    "train_model(data_url, n_estimator, max_depth, min_samples_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 2. Train a model with model tracking tools (CI)\n",
    "\n",
    "In this tutorial, we use mlflow as our model tracking tools. Before you start the phase 2, you need to launch the [mlflow](https://mlflow.org/) service in datalab.\n",
    "\n",
    "**Don't forget to disable IP protection**, otherwise you can not fetch trained model from the server.\n",
    "\n",
    "![mlflow_datalab.PNG](img/mlflow_datalab.PNG)\n",
    "\n",
    "Once you launched the mlflow service you need to create an experiment(project) if you don't have one. The name of the experiment is important, because we will need it to setup a mlflow context. The following code is an example on how to track your model and upload the information to mlflow server,\n",
    " 1. create a mlflow context\n",
    " 2. track training data source\n",
    " 3. track hyperparameter\n",
    " 4. track metric\n",
    " 5. track model binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_with_mlflow_tracking(mlflow_experiment_name: str, mlflow_run_name: str, data_url: str, n_estimator: int,\n",
    "                                     max_depth: int,\n",
    "                                     min_samples_split: int):\n",
    "    # Step1: Prepare data\n",
    "    feature_data, label_data = prepare_data(data_url)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(feature_data, label_data, train_size=0.8, test_size=0.2,\n",
    "                                                        random_state=0)\n",
    "    # set up mlflow context\n",
    "    mlflow.set_experiment(mlflow_experiment_name)\n",
    "    with mlflow.start_run(run_name=mlflow_run_name):\n",
    "        # create a random forest classifier\n",
    "        rf_clf = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth,\n",
    "                                        min_samples_split=min_samples_split,\n",
    "                                        n_jobs=2, random_state=0)\n",
    "        # train the model with training_data\n",
    "        rf_clf.fit(train_X, train_y)\n",
    "        # predict testing data\n",
    "        predicts_val = rf_clf.predict(test_X)\n",
    "\n",
    "        # Generate a cm\n",
    "        cm = confusion_matrix(test_y, predicts_val)\n",
    "        model_accuracy = get_model_accuracy(cm)\n",
    "        print(\"RandomForest model with hyper-parameters: (n_estimator=%f, max_depth=%f, min_samples_split=%f):\" % (\n",
    "            n_estimator, max_depth,\n",
    "            min_samples_split))\n",
    "        print(\"accuracy: %f\" % model_accuracy)\n",
    "        # log the model hyper-parameters to the mlflow server\n",
    "        mlflow.log_param(\"data_url\", data_url)\n",
    "        mlflow.log_param(\"n_estimator\", n_estimator)\n",
    "        mlflow.log_param(\"max_depth\", max_depth)\n",
    "        mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
    "\n",
    "        # log shap feature explanation extension. This will generate a graph of feature importance of the model\n",
    "        # mlflow.shap.log_explanation(rf_clf.predict, test_X.sample(70))\n",
    "\n",
    "        # log the model accuracy to the mlflow server\n",
    "        mlflow.log_metric(\"model_accuracy\", model_accuracy)\n",
    "\n",
    "        # log the model to the mlflow server\n",
    "        mlflow.sklearn.log_model(rf_clf, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To make the model training more flexible, we also convert the above jupyter notebook to a python script. Fot the full code, please check [here](tutorials/pokemon/train_model.py)\n",
    "\n",
    "And we write a little bash script to run the python with specific env var. **You need to modify the configuration such as MLFLOW_TRACKING_URI to your own mlflow server uri to make the script work**\n",
    "\n",
    "```shell\n",
    "#! /bin/bash\n",
    "export MLFLOW_S3_ENDPOINT_URL='https://minio.lab.sspcloud.fr'\n",
    "export MLFLOW_TRACKING_URI='https://user-pengfei-866801.kub.sspcloud.fr/'\n",
    "export MLFLOW_EXPERIMENT_NAME=\"pokemon\"\n",
    "\n",
    "run_name=\"default\"\n",
    "data_url=\"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "\n",
    "# set the hyper parameters\n",
    "n_estimator=\"50\"\n",
    "max_depth=\"30\"\n",
    "min_samples_split=\"2\"\n",
    "\n",
    "root_path=\"/home/jovyan/work/MLOPS\"\n",
    "\n",
    "python ${root_path}/tutorials/pokemon/train_model.py ${MLFLOW_EXPERIMENT_NAME} ${run_name} ${data_url} ${n_estimator} ${max_depth} ${min_samples_split}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! sh tutorials/pokemon/bash_command/local_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After you run the above command, all the tracking information of the model will be uploaded to the target mlflow server.\n",
    "\n",
    "Below figure shows the architecture:\n",
    "\n",
    "![local_training_archi.png](img/local_training_archi.png)\n",
    "\n",
    "But we still need to set up the python `virtual environment` and git clone the code, can we do better?\n",
    "\n",
    "Yes, we can. Thanks to the mlflow, which offers a launching API which can build the virtual environment and get the code automatically.\n",
    "\n",
    "We only need to setup two config files\n",
    "- MLproject (**This file must be at the root path of your git repo, otherwise mlflow can't run the workflow**)\n",
    "- conda.yaml\n",
    "\n",
    "Below is our [MLporject](MLproject):\n",
    "```yaml\n",
    "name: pokemon-legendary-estimator\n",
    "\n",
    "conda_env: tutorials/pokemon/conda.yaml\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      remote_server_uri: {type: str, default: http://pengfei.org:8000}\n",
    "      experiment_name: {type: str, default: test-1}\n",
    "      run_name: {type: str, default: default}\n",
    "      data_url: {type: str, default: https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv}\n",
    "      n_estimator: {type: int, default: 10}\n",
    "      max_depth: {type: int, default: 5}\n",
    "      min_samples_split: {type: int, default: 2}\n",
    "    command: \"python tutorials/pokemon/train_model.py {experiment_name} {run_name} {data_url} {n_estimator} {max_depth} {min_samples_split}\"\n",
    "```\n",
    "\n",
    "and [conda.yaml](tutorials/pokemon/conda.yaml)\n",
    "\n",
    "```yaml\n",
    "name: pokemon-legendary-estimator\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip\n",
    "  - pip:\n",
    "    - scikit-learn==1.1.2\n",
    "    - mlflow>=1.28.0\n",
    "    - pandas>=1.2.2\n",
    "    - numpy>=1.20.1\n",
    "    - shap>=0.39.0\n",
    "    - matplotlib>=3.4.1\n",
    "    - boto3==1.17.19\n",
    "```\n",
    "\n",
    "And now we can train a model without installing anything with below bash script. **You need to modify the configuration such as MLFLOW_TRACKING_URI to your own mlflow server uri to make the script work**\n",
    "\n",
    "```shell\n",
    "#! /bin/bash\n",
    "export MLFLOW_S3_ENDPOINT_URL='https://minio.lab.sspcloud.fr'\n",
    "export MLFLOW_TRACKING_URI='https://user-pengfei-866801.kub.sspcloud.fr/'\n",
    "export MLFLOW_EXPERIMENT_NAME=\"pokemon\"\n",
    "\n",
    "run_name=\"default\"\n",
    "data_url=\"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "\n",
    "# set the hyper parameters\n",
    "n_estimator=\"50\"\n",
    "max_depth=\"30\"\n",
    "min_samples_split=\"2\"\n",
    "\n",
    "mlflow run https://github.com/pengfei99/MLOPS.git -P remote_server_uri=${MLFLOW_TRACKING_URI} \\\n",
    "-P experiment_name=${MLFLOW_EXPERIMENT_NAME} \\\n",
    "-P data_url=${data_url} \\\n",
    "-P n_estimator=${n_estimator} -P max_depth=${max_depth} -P min_samples_split=${min_samples_split}\n",
    "```\n",
    "\n",
    "Now, you can see we let the mlflow download the code and create virtual environment for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! sh tutorials/pokemon/bash_command/remote_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 3 Train many models in parallel (CI)\n",
    "\n",
    "After Phase 2, we can track all information to reproduce a model, but it will take long time to compare many hyper-parameter combinations, if we can only train a model one by one. If we can train the model in parallel, then we can shorten the development time of the model.\n",
    "\n",
    "To do so, we need to launch a new service called [argo workflow](https://argoproj.github.io/argo-workflows/) in datalab\n",
    "\n",
    "![argo_datalab.PNG](img/argo_datalab.PNG)\n",
    "\n",
    "When you launch the argo-workflow service, pay attention to the `service account` value, because this service will need to launch new pods in the cluster k8s, which requires special rights, and this service account will give you the rights.\n",
    "\n",
    "Now let's check the workflow specification ([workflow.yaml](tutorials/pokemon/argo_workflow/workflow.yaml)).\n",
    "\n",
    "It can be dived into three parts:\n",
    "1. Workflow configuration\n",
    "2. Workflow dag planing\n",
    "3. Task logic implementation of the workflow dag\n",
    "\n",
    "**We need to configure the workflow parameter** before launching the workflow such as:\n",
    "- minio creds\n",
    "- mlflow uri\n",
    "- model git repo uri\n",
    "- etc.\n",
    "\n",
    "## 3.1 Installation of the argo workflow client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! sudo sh tutorials/pokemon/bash_command/argo_deb_install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the client version\n",
    "! argo version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! argo submit tutorials/pokemon/argo_workflow/workflow.yaml --watch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also check the workflow progress via the argo workflow web interface\n",
    "\n",
    "Below figure shows the architecture of what just happened\n",
    "![multi_model_training_archi_overview.png](img/multi_model_training_archi_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 4 Model management\n",
    "\n",
    "## 4.1 Model evaluation\n",
    "Now we have trained many models, we need to find the best model and deploy it into production.\n",
    "\n",
    "Mlflow allow us to compare model accuracy based on different hyperparameters. Below figure is an example\n",
    "\n",
    "![mlflow_model_eval.PNG](img/mlflow_model_eval.PNG)\n",
    "\n",
    "## 4.2 Model delivery\n",
    "\n",
    "After we find our target model, we can publish it to our model registry with a version number and state\n",
    "- production\n",
    "- staging\n",
    "- archived\n",
    "\n",
    "Below figure shows an example of the model registry\n",
    "![mlflow_model_version.PNG](img/mlflow_model_version.PNG)\n",
    "\n",
    "\n",
    "## 4.3 Consuming the model,\n",
    "\n",
    "Once the model is published in the model registry, it can be consumed by using the mlflow api. In below code, we build an rest Api by consuming \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_sample_data():\n",
    "    # get the data\n",
    "    data_url = \"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "    input_df = pd.read_csv(data_url, index_col=0)\n",
    "\n",
    "    ## prepare sample data\n",
    "    # Prepare data for ml model testing\n",
    "    legendary_pokemon = input_df[input_df[\"legendary\"] == True]\n",
    "    legendary_pokemon_sample = legendary_pokemon.sample(5).drop(['legendary', 'generation', 'total'],\n",
    "                                                                axis=1).select_dtypes(\n",
    "        exclude=['object'])\n",
    "    normal_pokemon = input_df[input_df[\"legendary\"] == False]\n",
    "    normal_pokemon_sample = normal_pokemon.sample(5).drop(['legendary', 'generation', 'total'], axis=1).select_dtypes(\n",
    "        exclude=['object'])\n",
    "    return legendary_pokemon_sample, normal_pokemon_sample\n",
    "\n",
    "\n",
    "# fetch the trained model from mlflow server by using its version\n",
    "def fetch_model(server_uri: str, experiment_name: str, model_version: str):\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = server_uri\n",
    "    model = mlflow.pyfunc.load_model(model_uri=f\"models:/{experiment_name}/{model_version}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(server_uri: str, experiment_name: str, model_version: str):\n",
    "    # step1: prepare sample data\n",
    "    legendary_sample, normal_sample = prepare_sample_data()\n",
    "\n",
    "    # step2: fetch the model\n",
    "    model = fetch_model(server_uri, experiment_name, model_version)\n",
    "    # step3: predict the sample data\n",
    "    print(model.predict(legendary_sample))\n",
    "    print(model.predict(normal_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022/08/31 13:04:10 WARNING mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment:\n",
      " - scikit-learn (current: 1.1.1, required: scikit-learn==0.24.1)\n",
      "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
      "2022/08/31 13:04:10 WARNING mlflow.pyfunc: The version of Python that the model was saved in, `Python 3.8.13`, differs from the version of Python that is currently running, `Python 3.9.12`, and may be incompatible\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.1 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.1 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "[False  True  True  True  True]\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "[False False False False False]\n"
     ]
    }
   ],
   "source": [
    "! sh tutorials/pokemon/bash_command/fetch_model_run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

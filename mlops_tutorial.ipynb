{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MLOps with datalab\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 What is MLOps?\n",
    "\n",
    "**MLOps** stands for `Machine Learning Operations`. It contains a set of best practices that seeks to increase automation and improve the efficiency of models development and deployment.\n",
    "\n",
    "### 1.2 Why do we need MLOps? Git is not enough?\n",
    "\n",
    "Put a machine learning model into production is difficult. It envoles many complex aspects such as\n",
    "- data collection/ingest,\n",
    "- data prep (e.g. cleaning, feature engineering, etc),\n",
    "- model development\n",
    "- model training,\n",
    "- model tuning\n",
    "- model deployment\n",
    "- model monitoring,\n",
    "- model explainability\n",
    "- ETC.\n",
    "\n",
    "Below figure shows the different aspects of mlops :\n",
    "\n",
    "![ml_technical_debt.PNG](img/ml_technical_debt.PNG)\n",
    "\n",
    "### 1.3 ML Operations\n",
    "\n",
    "We need to address the following MLOps principals:\n",
    "\n",
    "- **Model tracking**: track all the necessary element to reproduce the model such as code, hyperparameter and training data.\n",
    "- **Model review**: evaluate model and produce report.\n",
    "- **Model Governance** : manage model versions, model artifacts and transitions through their lifecycle (e.g. staging, production, archived,etc.).\n",
    "\n",
    "- **Model delivery/deployment**: Automate the process of model devlivery/deployment (e.g. permissions, server creation, API management, etc.)\n",
    "- **Model monitoring**: Monitor the state of model production server (e.g. number of request, response time, serving data anomalies, etc.)\n",
    "- **Model retraining**: Retrain model in case of **model drift** due to differences in training and inference data or `data evolution`.\n",
    "\n",
    "\n",
    "### 1.4 Continuous X\n",
    "\n",
    "- **CI**: Track all information of model (e.g. code, training data prepartion, hyper-parameters optimization)\n",
    "- **CD**: Deliver models with their complete training pipeline. \n",
    "- **CT(Continuous training)**: Models need to be retrained automatically. Because evolving data make your model decay. data validation is essential at this step, Because data drifting can be caused by evolution or errors.\n",
    "\n",
    "For more information about MLops, you can visit this [page](README.md) \n",
    "\n",
    "## 2 Illustrate mlops via a application example\n",
    "\n",
    "## 2.1 The context\n",
    "\n",
    "If you are a pokemon go player, when you capture a new pokemon, you may want to know if this pokemon is good or not. To make this easier, we would like to train a classification model that can tell us if the pokemon is legendary or not.\n",
    "\n",
    "In this tutorial we will use a random forest classifier to implement this model\n",
    "\n",
    "## 2.2 Prepare environment and install the dependencies\n",
    "\n",
    "Launch a jupyter service in datalab.\n",
    "\n",
    "**Don't forget to assign admin role in kubernetes tab**\n",
    "\n",
    "![jupyter_datalab.PNG](img/jupyter_datalab.PNG)\n",
    "\n",
    "Start a terminal in jupyter\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/pengfei99/MLOPS.git\n",
    "```\n",
    "\n",
    "Then install the dependencies\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0. EDA and feature engnieering on training data\n",
    "\n",
    "After EDA and feature engnieering, we select below colums as features\n",
    "- hp\n",
    "- attack\n",
    "- defence\n",
    "- special_attack\n",
    "- special_defenece\n",
    "- speed\n",
    "\n",
    "and column **legendary** as label.\n",
    "\n",
    "For more details on feature engnieering, please visit this [repo](https://github.com/pengfei99/FeatureEngineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the training data path\n",
    "data_url = \"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "raw_df = pd.read_csv(data_url, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 12)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name               object\n",
       "type_1             object\n",
       "type_2             object\n",
       "total               int64\n",
       "hp                  int64\n",
       "attack              int64\n",
       "defense             int64\n",
       "special_attack      int64\n",
       "special_defense     int64\n",
       "speed               int64\n",
       "generation          int64\n",
       "legendary            bool\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type_1</th>\n",
       "      <th>type_2</th>\n",
       "      <th>total</th>\n",
       "      <th>hp</th>\n",
       "      <th>attack</th>\n",
       "      <th>defense</th>\n",
       "      <th>special_attack</th>\n",
       "      <th>special_defense</th>\n",
       "      <th>speed</th>\n",
       "      <th>generation</th>\n",
       "      <th>legendary</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>318</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>405</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>525</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>625</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>309</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name type_1  type_2  total  hp  attack  defense  \\\n",
       "index                                                                     \n",
       "1                  Bulbasaur  Grass  Poison    318  45      49       49   \n",
       "2                    Ivysaur  Grass  Poison    405  60      62       63   \n",
       "3                   Venusaur  Grass  Poison    525  80      82       83   \n",
       "3      VenusaurMega Venusaur  Grass  Poison    625  80     100      123   \n",
       "4                 Charmander   Fire     NaN    309  39      52       43   \n",
       "\n",
       "       special_attack  special_defense  speed  generation  legendary  \n",
       "index                                                                 \n",
       "1                  65               65     45           1      False  \n",
       "2                  80               80     60           1      False  \n",
       "3                 100              100     80           1      False  \n",
       "3                 122              120     80           1      False  \n",
       "4                  60               50     65           1      False  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 1. Train a model in an old school way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# calculate an accuracy from the confusion matrix\n",
    "def get_model_accuracy(cf_matrix):\n",
    "    diagonal_sum = cf_matrix.trace()\n",
    "    sum_of_all_elements = cf_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements\n",
    "\n",
    "# main logic for model training\n",
    "def train_model(data_url: str, n_estimator: int, max_depth: int, min_samples_split: int):\n",
    "    print(f\"data source: {data_url}\")\n",
    "    feature_data, label_data = prepare_data(data_url)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(feature_data, label_data, train_size=0.8, test_size=0.2,\n",
    "                                                        random_state=0)\n",
    "    # print(len(test_X))\n",
    "\n",
    "    # create a random forest classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth,\n",
    "                                    min_samples_split=min_samples_split,\n",
    "                                    n_jobs=2, random_state=0)\n",
    "    # train the model with training_data\n",
    "    rf_clf.fit(train_X, train_y)\n",
    "    # predict testing data\n",
    "    predicts_val = rf_clf.predict(test_X)\n",
    "\n",
    "    # Generate a cm\n",
    "    cm = confusion_matrix(test_y, predicts_val)\n",
    "    model_accuracy = get_model_accuracy(cm)\n",
    "    print(\"RandomForest model with hyper-parameters: (n_estimator=%f, max_depth=%f, min_samples_split=%f):\" % (\n",
    "        n_estimator, max_depth, min_samples_split))\n",
    "    print(\"accuracy: %f\" % model_accuracy)\n",
    "\n",
    "\n",
    "# data preparation\n",
    "def prepare_data(data_url):\n",
    "    # read data as df\n",
    "    try:\n",
    "        input_df = pd.read_csv(data_url, index_col=0)\n",
    "        input_df.head()\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Unable to read data from the giving path, check your data location. Error: %s\", e\n",
    "        )\n",
    "    # Prepare data for ml model\n",
    "    label = input_df.legendary\n",
    "    feature = input_df.drop(['legendary', 'generation', 'total'], axis=1).select_dtypes(exclude=['object'])\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data source: https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\n",
      "RandomForest model with hyper-parameters: (n_estimator=50.000000, max_depth=50.000000, min_samples_split=2.000000):\n",
      "accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "\n",
    "# set the hyper parameters\n",
    "n_estimator = 50 # The number of trees in the forest.\n",
    "max_depth = 50 # The maximum depth of the tree\n",
    "min_samples_split = 2 # The minimum number of samples required to split an internal node\n",
    "\n",
    "# train the model\n",
    "train_model(data_url, n_estimator, max_depth, min_samples_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 2. Train a model with model tracking tools (CI)\n",
    "\n",
    "In this tutorial, we use mlflow as our model tracking tools. Before you start the phase 2, you need to launch the [mlflow](https://mlflow.org/) service in datalab.\n",
    "\n",
    "\n",
    "![mlflow_datalab.PNG](img/mlflow_datalab.PNG)\n",
    "\n",
    "**Don't forget to disable IP protection**, otherwise you can not fetch trained model from the server.\n",
    "![mlflow_ip_protection.PNG](img/mlflow_ip_protection.PNG)\n",
    "\n",
    "Once you launched the mlflow service you need to create an experiment(project) if you don't have one. The name of the experiment is important, because we will need it to setup a mlflow context. The following code is an example on how to track your model and upload the information to mlflow server,\n",
    " 1. create a mlflow context\n",
    " 2. track training data source\n",
    " 3. track hyperparameter\n",
    " 4. track metric\n",
    " 5. track model binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model_with_mlflow_tracking(mlflow_experiment_name: str, mlflow_run_name: str, data_url: str, n_estimator: int,\n",
    "                                     max_depth: int,\n",
    "                                     min_samples_split: int):\n",
    "    # Step1: Prepare data\n",
    "    feature_data, label_data = prepare_data(data_url)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(feature_data, label_data, train_size=0.8, test_size=0.2,\n",
    "                                                        random_state=0)\n",
    "    # set up mlflow context\n",
    "    mlflow.set_experiment(mlflow_experiment_name)\n",
    "    with mlflow.start_run(run_name=mlflow_run_name):\n",
    "        # create a random forest classifier\n",
    "        rf_clf = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth,\n",
    "                                        min_samples_split=min_samples_split,\n",
    "                                        n_jobs=2, random_state=0)\n",
    "        # train the model with training_data\n",
    "        rf_clf.fit(train_X, train_y)\n",
    "        # predict testing data\n",
    "        predicts_val = rf_clf.predict(test_X)\n",
    "\n",
    "        # Generate a cm\n",
    "        cm = confusion_matrix(test_y, predicts_val)\n",
    "        model_accuracy = get_model_accuracy(cm)\n",
    "        print(\"RandomForest model with hyper-parameters: (n_estimator=%f, max_depth=%f, min_samples_split=%f):\" % (\n",
    "            n_estimator, max_depth,\n",
    "            min_samples_split))\n",
    "        print(\"accuracy: %f\" % model_accuracy)\n",
    "        # log the model hyper-parameters to the mlflow server\n",
    "        mlflow.log_param(\"data_url\", data_url)\n",
    "        mlflow.log_param(\"n_estimator\", n_estimator)\n",
    "        mlflow.log_param(\"max_depth\", max_depth)\n",
    "        mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
    "\n",
    "        # log shap feature explanation extension. This will generate a graph of feature importance of the model\n",
    "        # mlflow.shap.log_explanation(rf_clf.predict, test_X.sample(70))\n",
    "\n",
    "        # log the model accuracy to the mlflow server\n",
    "        mlflow.log_metric(\"model_accuracy\", model_accuracy)\n",
    "\n",
    "        # log the model to the mlflow server\n",
    "        mlflow.sklearn.log_model(rf_clf, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To make the model training more flexible, we also convert the above jupyter notebook to a python script. Fot the full code, please check [here](tutorials/pokemon/train_model.py)\n",
    "\n",
    "And we write a little bash script to run the python with specific env var. You can edit the script [here](tutorials/pokemon/bash_command/local_run.sh).\n",
    "\n",
    "**You need to modify the configuration such as MLFLOW_TRACKING_URI to your own mlflow server uri to make the script work**\n",
    "\n",
    "```shell\n",
    "#! /bin/bash\n",
    "export MLFLOW_S3_ENDPOINT_URL='https://minio.lab.sspcloud.fr'\n",
    "export MLFLOW_TRACKING_URI='https://user-pengfei-866801.kub.sspcloud.fr/'\n",
    "export MLFLOW_EXPERIMENT_NAME=\"pokemon\"\n",
    "\n",
    "run_name=\"default\"\n",
    "data_url=\"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "\n",
    "# set the hyper parameters\n",
    "n_estimator=\"50\"\n",
    "max_depth=\"30\"\n",
    "min_samples_split=\"2\"\n",
    "\n",
    "root_path=\"/home/jovyan/work/MLOPS\"\n",
    "\n",
    "python ${root_path}/tutorials/pokemon/train_model.py ${MLFLOW_EXPERIMENT_NAME} ${run_name} ${data_url} ${n_estimator} ${max_depth} ${min_samples_split}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest model with hyper-parameters: (n_estimator=50.000000, max_depth=30.000000, min_samples_split=2.000000):\n",
      "accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "! sh tutorials/pokemon/bash_command/local_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After you run the above command, all the tracking information of the model will be uploaded to the target mlflow server.\n",
    "\n",
    "Below figure shows the architecture:\n",
    "\n",
    "![local_training_archi.png](img/local_training_archi.png)\n",
    "\n",
    "But we still need to set up the python `virtual environment` and git clone the code, can we do better?\n",
    "\n",
    "Yes, we can. Thanks to the mlflow, which offers a launching API which can build the virtual environment and get the code automatically.\n",
    "\n",
    "We only need to setup two config files\n",
    "- MLproject (**This file must be at the root path of your git repo, otherwise mlflow can't run the workflow**)\n",
    "- conda.yaml\n",
    "\n",
    "Below is our [MLporject](MLproject):\n",
    "```yaml\n",
    "name: pokemon-legendary-estimator\n",
    "\n",
    "conda_env: tutorials/pokemon/conda.yaml\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      remote_server_uri: {type: str, default: http://pengfei.org:8000}\n",
    "      experiment_name: {type: str, default: test-1}\n",
    "      run_name: {type: str, default: default}\n",
    "      data_url: {type: str, default: https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv}\n",
    "      n_estimator: {type: int, default: 10}\n",
    "      max_depth: {type: int, default: 5}\n",
    "      min_samples_split: {type: int, default: 2}\n",
    "    command: \"python tutorials/pokemon/train_model.py {experiment_name} {run_name} {data_url} {n_estimator} {max_depth} {min_samples_split}\"\n",
    "```\n",
    "\n",
    "and [conda.yaml](tutorials/pokemon/conda.yaml)\n",
    "\n",
    "```yaml\n",
    "name: pokemon-legendary-estimator\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip\n",
    "  - pip:\n",
    "    - scikit-learn==1.1.2\n",
    "    - mlflow>=1.28.0\n",
    "    - pandas>=1.2.2\n",
    "    - numpy>=1.20.1\n",
    "    - shap>=0.39.0\n",
    "    - matplotlib>=3.4.1\n",
    "    - boto3==1.17.19\n",
    "```\n",
    "\n",
    "And now we can train a model without installing anything with below bash script. You can edit the script [here](tutorials/pokemon/bash_command/remote_run.sh).\n",
    "\n",
    "**You need to replace the configuration such as MLFLOW_TRACKING_URI to your own mlflow server uri to make the script work**\n",
    "\n",
    "```shell\n",
    "#! /bin/bash\n",
    "export MLFLOW_S3_ENDPOINT_URL='https://minio.lab.sspcloud.fr'\n",
    "export MLFLOW_TRACKING_URI='https://user-pengfei-866801.kub.sspcloud.fr/'\n",
    "export MLFLOW_EXPERIMENT_NAME=\"pokemon\"\n",
    "\n",
    "run_name=\"default\"\n",
    "data_url=\"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "\n",
    "# set the hyper parameters\n",
    "n_estimator=\"50\"\n",
    "max_depth=\"30\"\n",
    "min_samples_split=\"2\"\n",
    "\n",
    "mlflow run https://github.com/pengfei99/MLOPS.git -P remote_server_uri=${MLFLOW_TRACKING_URI} \\\n",
    "-P experiment_name=${MLFLOW_EXPERIMENT_NAME} \\\n",
    "-P data_url=${data_url} \\\n",
    "-P n_estimator=${n_estimator} -P max_depth=${max_depth} -P min_samples_split=${min_samples_split}\n",
    "```\n",
    "\n",
    "Now, you can see we let the mlflow download the code and create virtual environment for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022/09/19 11:19:37 INFO mlflow.projects.utils: === Fetching project from https://github.com/pengfei99/MLOPS.git into /tmp/tmp_4zldo_s ===\n",
      "2022/09/19 11:19:39 INFO mlflow.projects.utils: Fetched 'main' branch\n",
      "2022/09/19 11:19:41 INFO mlflow.utils.conda: === Creating conda environment mlflow-fa0bb422694da7444e20b71fef0b188b71acc4de ===\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.14.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "xz-5.2.5             | 339 KB    | ##################################### | 100% \n",
      "tk-8.6.12            | 3.0 MB    | ##################################### | 100% \n",
      "libgomp-11.2.0       | 474 KB    | ##################################### | 100% \n",
      "libffi-3.3           | 50 KB     | ##################################### | 100% \n",
      "ncurses-6.3          | 781 KB    | ##################################### | 100% \n",
      "wheel-0.37.1         | 33 KB     | ##################################### | 100% \n",
      "openssl-1.1.1q       | 2.5 MB    | ##################################### | 100% \n",
      "libgcc-ng-11.2.0     | 5.3 MB    | ##################################### | 100% \n",
      "sqlite-3.39.2        | 1.1 MB    | ##################################### | 100% \n",
      "setuptools-63.4.1    | 1.1 MB    | ##################################### | 100% \n",
      "libstdcxx-ng-11.2.0  | 4.7 MB    | ##################################### | 100% \n",
      "certifi-2022.6.15    | 153 KB    | ##################################### | 100% \n",
      "pip-22.1.2           | 2.5 MB    | ##################################### | 100% \n",
      "ld_impl_linux-64-2.3 | 654 KB    | ##################################### | 100% \n",
      "python-3.8.13        | 18.8 MB   | ##################################### | 100% \n",
      "_openmp_mutex-5.1    | 21 KB     | ##################################### | 100% \n",
      "ca-certificates-2022 | 124 KB    | ##################################### | 100% \n",
      "readline-8.1.2       | 354 KB    | ##################################### | 100% \n",
      "zlib-1.2.12          | 103 KB    | ##################################### | 100% \n",
      "_libgcc_mutex-0.1    | 3 KB      | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Installing pip dependencies: - Ran pip subprocess with arguments:\n",
      "['/opt/conda/envs/mlflow-fa0bb422694da7444e20b71fef0b188b71acc4de/bin/python', '-m', 'pip', 'install', '-U', '-r', '/tmp/tmp_4zldo_s/tutorials/pokemon/condaenv.2h3h9b1y.requirements.txt']\n",
      "Pip subprocess output:\n",
      "Collecting scikit-learn==1.1.2\n",
      "  Downloading scikit_learn-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 31.2/31.2 MB 12.7 MB/s eta 0:00:00\n",
      "Collecting mlflow>=1.28.0\n",
      "  Using cached mlflow-1.28.0-py3-none-any.whl (17.0 MB)\n",
      "Collecting pandas>=1.2.2\n",
      "  Downloading pandas-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.7/11.7 MB 15.8 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.20.1\n",
      "  Downloading numpy-1.23.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 15.8 MB/s eta 0:00:00\n",
      "Collecting shap>=0.39.0\n",
      "  Downloading shap-0.41.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (575 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 575.9/575.9 kB 17.5 MB/s eta 0:00:00\n",
      "Collecting matplotlib>=3.4.1\n",
      "  Downloading matplotlib-3.6.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.4/9.4 MB 17.9 MB/s eta 0:00:00\n",
      "Collecting boto3==1.17.19\n",
      "  Downloading boto3-1.17.19-py2.py3-none-any.whl (130 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.3/130.3 kB 30.7 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.3.2\n",
      "  Downloading scipy-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.4/43.4 MB 12.3 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.0.0\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 46.5 MB/s eta 0:00:00\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.21.0,>=1.20.19\n",
      "  Downloading botocore-1.20.112-py2.py3-none-any.whl (7.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 21.0 MB/s eta 0:00:00\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.4/73.4 kB 5.4 MB/s eta 0:00:00\n",
      "Collecting gitpython<4,>=2.1.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.2/181.2 kB 15.2 MB/s eta 0:00:00\n",
      "Collecting docker<6,>=4.0.0\n",
      "  Using cached docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "Collecting sqlparse<1,>=0.4.0\n",
      "  Using cached sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "Collecting packaging<22\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 13.3 MB/s eta 0:00:00\n",
      "Collecting pytz<2023\n",
      "  Downloading pytz-2022.2.1-py2.py3-none-any.whl (500 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 500.6/500.6 kB 11.8 MB/s eta 0:00:00\n",
      "Collecting gunicorn<21\n",
      "  Using cached gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
      "Collecting databricks-cli<1,>=0.8.7\n",
      "  Using cached databricks-cli-0.17.3.tar.gz (77 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting click<9,>=7.0\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 kB 21.4 MB/s eta 0:00:00\n",
      "Collecting alembic<2\n",
      "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.8/209.8 kB 14.9 MB/s eta 0:00:00\n",
      "Collecting Flask<3\n",
      "  Using cached Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "Collecting requests<3,>=2.17.3\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 17.1 MB/s eta 0:00:00\n",
      "Collecting querystring-parser<2\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting entrypoints<1\n",
      "  Downloading entrypoints-0.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting protobuf<5,>=3.12.0\n",
      "  Downloading protobuf-4.21.6-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 408.4/408.4 kB 17.6 MB/s eta 0:00:00\n",
      "Collecting prometheus-flask-exporter<1\n",
      "  Using cached prometheus_flask_exporter-0.20.3-py3-none-any.whl (18 kB)\n",
      "Collecting sqlalchemy<2,>=1.4.0\n",
      "  Downloading SQLAlchemy-1.4.41-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 17.0 MB/s eta 0:00:00\n",
      "Collecting cloudpickle<3\n",
      "  Downloading cloudpickle-2.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting pyyaml<7,>=5.1\n",
      "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 701.2/701.2 kB 12.1 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata!=4.7.0,<5,>=3.7.0\n",
      "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting python-dateutil>=2.8.1\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 13.4 MB/s eta 0:00:00\n",
      "Collecting tqdm>4.25.0\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.5/78.5 kB 31.1 MB/s eta 0:00:00\n",
      "Collecting numba\n",
      "  Downloading numba-0.56.2-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.5/3.5 MB 17.8 MB/s eta 0:00:00\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 14.5 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 19.5 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 26.5 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.37.2-py3-none-any.whl (959 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 959.8/959.8 kB 18.9 MB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (295 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.9/295.9 kB 18.2 MB/s eta 0:00:00\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.2-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.7/78.7 kB 25.2 MB/s eta 0:00:00\n",
      "Collecting importlib-resources\n",
      "  Downloading importlib_resources-5.9.0-py3-none-any.whl (33 kB)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.4/140.4 kB 24.7 MB/s eta 0:00:00\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Downloading PyJWT-2.5.0-py3-none-any.whl (20 kB)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Downloading oauthlib-3.2.1-py3-none-any.whl (151 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 30.7 MB/s eta 0:00:00\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting six>=1.10.0\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-1.4.1-py3-none-any.whl (55 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.0/55.0 kB 12.9 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.0\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting Jinja2>=3.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 kB 21.6 MB/s eta 0:00:00\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Using cached Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.1/63.1 kB 20.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools>=3.0 in /opt/conda/envs/mlflow-fa0bb422694da7444e20b71fef0b188b71acc4de/lib/python3.8/site-packages (from gunicorn<21->mlflow>=1.28.0->-r /tmp/tmp_4zldo_s/tutorials/pokemon/condaenv.2h3h9b1y.requirements.txt (line 2)) (63.4.1)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting prometheus-client\n",
      "  Downloading prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 23.7 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 kB 21.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/mlflow-fa0bb422694da7444e20b71fef0b188b71acc4de/lib/python3.8/site-packages (from requests<3,>=2.17.3->mlflow>=1.28.0->-r /tmp/tmp_4zldo_s/tutorials/pokemon/condaenv.2h3h9b1y.requirements.txt (line 2)) (2022.6.15)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-1.1.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 157.1/157.1 kB 23.1 MB/s eta 0:00:00\n",
      "Collecting setuptools>=3.0\n",
      "  Downloading setuptools-59.8.0-py3-none-any.whl (952 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 952.8/952.8 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Downloading llvmlite-0.39.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.6/34.6 MB 14.8 MB/s eta 0:00:00\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Building wheels for collected packages: databricks-cli\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.17.3-py3-none-any.whl size=139084 sha256=4353bbc1879e5a54ccf99d05148d5c11fd808916686be0aaeebf9edcd7f8f371\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/58/40/7c/d021d51dac18bfd095fb6837572ad2e6f1a34d221f4b1d976b\n",
      "Successfully built databricks-cli\n",
      "Installing collected packages: pytz, zipp, websocket-client, urllib3, tqdm, threadpoolctl, tabulate, sqlparse, smmap, slicer, six, setuptools, pyyaml, pyparsing, pyjwt, protobuf, prometheus-client, pillow, oauthlib, numpy, MarkupSafe, llvmlite, kiwisolver, joblib, jmespath, itsdangerous, idna, greenlet, fonttools, entrypoints, cycler, cloudpickle, click, charset-normalizer, Werkzeug, sqlalchemy, scipy, requests, querystring-parser, python-dateutil, packaging, Mako, Jinja2, importlib-resources, importlib-metadata, gunicorn, gitdb, contourpy, scikit-learn, pandas, numba, matplotlib, gitpython, Flask, docker, databricks-cli, botocore, alembic, shap, s3transfer, prometheus-flask-exporter, mlflow, boto3\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 63.4.1\n",
      "    Uninstalling setuptools-63.4.1:\n",
      "      Successfully uninstalled setuptools-63.4.1\n",
      "Successfully installed Flask-2.2.2 Jinja2-3.1.2 Mako-1.2.2 MarkupSafe-2.1.1 Werkzeug-2.2.2 alembic-1.8.1 boto3-1.17.19 botocore-1.20.112 charset-normalizer-2.1.1 click-8.1.3 cloudpickle-2.2.0 contourpy-1.0.5 cycler-0.11.0 databricks-cli-0.17.3 docker-5.0.3 entrypoints-0.4 fonttools-4.37.2 gitdb-4.0.9 gitpython-3.1.27 greenlet-1.1.3 gunicorn-20.1.0 idna-3.4 importlib-metadata-4.12.0 importlib-resources-5.9.0 itsdangerous-2.1.2 jmespath-0.10.0 joblib-1.2.0 kiwisolver-1.4.4 llvmlite-0.39.1 matplotlib-3.6.0 mlflow-1.28.0 numba-0.56.2 numpy-1.23.3 oauthlib-3.2.1 packaging-21.3 pandas-1.4.4 pillow-9.2.0 prometheus-client-0.14.1 prometheus-flask-exporter-0.20.3 protobuf-4.21.6 pyjwt-2.5.0 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2022.2.1 pyyaml-6.0 querystring-parser-1.2.4 requests-2.28.1 s3transfer-0.3.7 scikit-learn-1.1.2 scipy-1.9.1 setuptools-59.8.0 shap-0.41.0 six-1.16.0 slicer-0.0.7 smmap-5.0.0 sqlalchemy-1.4.41 sqlparse-0.4.2 tabulate-0.8.10 threadpoolctl-3.1.0 tqdm-4.64.1 urllib3-1.26.12 websocket-client-1.4.1 zipp-3.8.1\n",
      "\n",
      "done\n",
      "#\n",
      "# To activate this environment, use\n",
      "#\n",
      "#     $ conda activate mlflow-fa0bb422694da7444e20b71fef0b188b71acc4de\n",
      "#\n",
      "# To deactivate an active environment, use\n",
      "#\n",
      "#     $ conda deactivate\n",
      "\n",
      "2022/09/19 11:21:46 INFO mlflow.projects.utils: === Created directory /tmp/tmpnhcq52ni for downloading remote URIs passed to arguments of type 'path' ===\n",
      "2022/09/19 11:21:46 INFO mlflow.projects.backend.local: === Running command 'source activate mlflow-fa0bb422694da7444e20b71fef0b188b71acc4de 1>&2 && python tutorials/pokemon/train_model.py pokemon default https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv 50 30 2' in run with ID '823d5feed7304758b75e86f9116ef49b' === \n",
      "RandomForest model with hyper-parameters: (n_estimator=50.000000, max_depth=30.000000, min_samples_split=2.000000):\n",
      "accuracy: 0.925000\n",
      "2022/09/19 11:21:51 INFO mlflow.projects: === Run (ID '823d5feed7304758b75e86f9116ef49b') succeeded ===\n"
     ]
    }
   ],
   "source": [
    "! sh tutorials/pokemon/bash_command/remote_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 3 Train many models in parallel (CI)\n",
    "\n",
    "After Phase 2, we can track all information to reproduce a model, but it will take long time to compare many hyper-parameter combinations, if we can only train a model one by one. If we can train the model in parallel, then we can shorten the development time of the model.\n",
    "\n",
    "To do so, we need to launch a new service called [argo workflow](https://argoproj.github.io/argo-workflows/) in datalab\n",
    "\n",
    "![argo_datalab.PNG](img/argo_datalab.PNG)\n",
    "\n",
    "When you launch the argo-workflow service, pay attention to the `service account` value, because this service will need to launch new pods in the cluster k8s, which requires special rights, and this service account will give you the rights.\n",
    "\n",
    "Now let's check the workflow specification ([workflow.yaml](tutorials/pokemon/argo_workflow/workflow.yaml)).\n",
    "\n",
    "It can be dived into three parts:\n",
    "1. Workflow configuration\n",
    "2. Workflow dag planing\n",
    "3. Task logic implementation of the workflow dag\n",
    "\n",
    "**We need to configure the workflow parameter** before launching the workflow such as:\n",
    "- minio creds\n",
    "- mlflow uri\n",
    "- model git repo uri\n",
    "- etc.\n",
    "\n",
    "## 3.1 Installation of the argo workflow client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! sudo sh tutorials/pokemon/bash_command/argo_deb_install.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# check the client version\n",
    "! argo version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3.2 Run the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! argo submit tutorials/pokemon/argo_workflow/workflow.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also check the workflow progress via the argo workflow web interface\n",
    "\n",
    "Below figure shows the architecture of what just happened\n",
    "![multi_model_training_archi_overview.png](img/multi_model_training_archi_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase 4 Model management\n",
    "\n",
    "## 4.1 Model evaluation\n",
    "Now we have trained many models, we need to find the best model and deploy it into production.\n",
    "\n",
    "Mlflow allow us to compare model accuracy based on different hyperparameters. Below figure is an example\n",
    "\n",
    "![mlflow_model_eval.PNG](img/mlflow_model_eval.PNG)\n",
    "\n",
    "## 4.2 Model delivery\n",
    "\n",
    "After we find our target model, we can publish it to our model registry with a version number and state\n",
    "- production\n",
    "- staging\n",
    "- archived\n",
    "\n",
    "Below figure shows an example of the model registry\n",
    "![mlflow_model_version.PNG](img/mlflow_model_version.PNG)\n",
    "\n",
    "\n",
    "## 4.3 Consuming the model,\n",
    "\n",
    "Once the model is published in the model registry, it can be consumed by using the mlflow api. In below code, we use the `mlflow.pyfunc.load_model` function to fetch a trained model from the mlflow server.\n",
    "\n",
    "The complete source code of `fetch_model.py` can be found [here]()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_sample_data():\n",
    "    # get the data\n",
    "    data_url = \"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "    input_df = pd.read_csv(data_url, index_col=0)\n",
    "\n",
    "    ## prepare sample data\n",
    "    # Prepare data for ml model testing\n",
    "    legendary_pokemon = input_df[input_df[\"legendary\"] == True]\n",
    "    legendary_pokemon_sample = legendary_pokemon.sample(5).drop(['legendary', 'generation', 'total'],\n",
    "                                                                axis=1).select_dtypes(\n",
    "        exclude=['object'])\n",
    "    normal_pokemon = input_df[input_df[\"legendary\"] == False]\n",
    "    normal_pokemon_sample = normal_pokemon.sample(5).drop(['legendary', 'generation', 'total'], axis=1).select_dtypes(\n",
    "        exclude=['object'])\n",
    "    return legendary_pokemon_sample, normal_pokemon_sample\n",
    "\n",
    "\n",
    "# fetch the trained model from mlflow server by using its version\n",
    "def fetch_model(server_uri: str, experiment_name: str, model_version: str):\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = server_uri\n",
    "    model = mlflow.pyfunc.load_model(model_uri=f\"models:/{experiment_name}/{model_version}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(server_uri: str, experiment_name: str, model_version: str):\n",
    "    # step1: prepare sample data\n",
    "    legendary_sample, normal_sample = prepare_sample_data()\n",
    "\n",
    "    # step2: fetch the model\n",
    "    model = fetch_model(server_uri, experiment_name, model_version)\n",
    "    # step3: predict the sample data\n",
    "    print(f\"The prediction of 5 five legendary pokemon: {model.predict(legendary_sample)}\")\n",
    "    print(f\"The prediction of 5 five normal pokemon: {model.predict(normal_sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! sh tutorials/pokemon/bash_command/fetch_model_run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 5 Model deployment (CD)\n",
    "\n",
    "In the above section, we have seen how to consume a published model. But we have to still run the code, it's not very practical for the users. So we want to expose our model as a Rest Api and when the model is updated, the api is updated automatically.\n",
    "\n",
    "## 5.1 First step: Build Rest Api\n",
    "We need to build a Rest Api by fetching the trained model, it's not the main goal of this tutorial. For more information on how to build a Rest Api, you can visit this [git repo](https://github.com/pengfei99/PokemonDTCAPI.git)\n",
    "\n",
    "## 5.2 Second step: Create a k8s deployment of the Rest Api\n",
    "\n",
    "Now we need to create a k8s deployment of the Rest api. The deployment configuration contais three files:\n",
    "- deployment.yaml : It runs multiple replicas of our **model API** and automatically replaces any instances that fail or become unresponsive.\n",
    "- service.yaml : It exposes our model API on Pods as a network service.\n",
    "- ingress.yaml : It manages external access to the services via HTTP (e.g. url, ssl, etc.).\n",
    "\n",
    "You can find the full code [here](tutorials/pokemon/k8s).\n",
    "\n",
    "**Don't forget to change the mlflow server uri** in the deployment.yaml\n",
    "\n",
    "Now we can try to deploy the model api by using below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! kubectl apply -f tutorials/pokemon/k8s/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If you want to delete it, you can run below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! kubectl delete -f tutorials/pokemon/k8s/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5.3 Third step: Automate the deployment\n",
    "\n",
    "We have finished the manual deployment, now we want to deploy the model automatically when a new model is published.\n",
    "\n",
    "We will need to launch a new service called [argo CD](https://argo-cd.readthedocs.io/en/stable/)\n",
    "\n",
    "![argo_datalab.PNG](img/argo_datalab.PNG)\n",
    "\n",
    "Once you the argo CD is launched, you can create a new app in it.\n",
    "\n",
    "Below is an example of the yaml file to create a new app. You can find it [here](tutorials/pokemon/argocd)\n",
    "\n",
    "```yaml\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: Application\n",
    "metadata:\n",
    "  name: pokemon-api\n",
    "spec:\n",
    "  project: default\n",
    "  source:\n",
    "    repoURL: https://github.com/pengfei99/MLOPS.git\n",
    "    targetRevision: HEAD\n",
    "    path: tutorials/pokemon/k8s\n",
    "  destination:\n",
    "    server: https://kubernetes.default.svc\n",
    "    namespace: user-pengfei\n",
    "  syncPolicy:\n",
    "    automated:\n",
    "      selfHeal: true\n",
    "```\n",
    "\n",
    "There are two important part you need to change:\n",
    "1. source: Your repo git url and the path of your k8s deployment code\n",
    "2. destination: k8s api server url and the target namespace\n",
    "\n",
    "After you created the application in argo CD with above config, you should see something like in the below figure\n",
    "![argo_cd_datalab.PNG](img/argo_cd_datalab.PNG)\n",
    "Now, argo CD will watch your git repo, every 5 mins, it will compare the code in `tutorials/pokemon/k8s` (git repo) with the currently deployed version. If an update is detected, argo CD will deploy the updated model automatically for us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. It's your turn now\n",
    "\n",
    "Try to reproduce the above step and deploy your own model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

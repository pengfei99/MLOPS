{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MLOps with datalab\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1 What is MLOps?\n",
    "\n",
    "**MLOps** stands for `Machine Learning Operations`. It contains a set of best practices that seeks to increase automation and improve the efficiency of models development and deployment.\n",
    "\n",
    "### 1.2 Why do we need MLOps? Git is not enough?\n",
    "\n",
    "Put a machine learning model into production is difficult. It envoles many complex components such as\n",
    "- data collection/ingest,\n",
    "- data prep (e.g. cleaning, feature engineering, etc),\n",
    "- model development\n",
    "- model training,\n",
    "- model tuning\n",
    "- model deployment\n",
    "- model monitoring,\n",
    "- model explainability\n",
    "- ETC.\n",
    "\n",
    "Below figure shows the mlops competence requirement:\n",
    "\n",
    "![ml_technical_debt.PNG](img/ml_technical_debt.PNG)\n",
    "\n",
    "### 1.3 ML Operations\n",
    "\n",
    "We need to address the following MLOps principals:\n",
    "\n",
    "- **Model tracking**: track all the necessary element to reproduce the model such as code, hyperparameter and training data.\n",
    "- **Model review**: Test model and produce quality assurance report. Inference model production-specifics properties such as model response times.\n",
    "- **Model Governance** : manage model versions, model artifacts and transitions through their lifecycle (e.g. staging, production, archived,etc.).\n",
    "\n",
    "- **Model deployment**: Automate the process of deploying registered models (e.g. permissions, cluster creation, API management, etc.)\n",
    "- **Model monitoring**: Monitor the state of model production server (e.g. number of request, response time, serving data anomalies, etc.)\n",
    "- **Model retraining**: Create alerts and automation to take corrective action in case of **model drift** due to\n",
    "                    differences in training and inference data or `data evolution`.\n",
    "\n",
    "\n",
    "### 1.4 Continuous X\n",
    "\n",
    "- **CI**: Track model code, training data( e.g. Feature engineering/selection), hyper-parameters optimization\n",
    "- **CD**: Need to deliver not only an executable package, but also a complete pipeline of how the model is trained.\n",
    "- **CT(Continuous training)**: Models need to be retrained automatically. Because evolving data make your model decay. data validation is essential at this step, Because data drifting can be caused by evolution or errors.\n",
    "\n",
    "## 2 Illustrate mlops via a application example\n",
    "\n",
    "## 2.1 The context\n",
    "\n",
    "If you are a pokemon go player, when you capture a new pokemon, you may want to know if this pokemon is good or not. To make this easier, we would like to train a classification model that can tell us if the pokemon is legendary or not.\n",
    "\n",
    "In this tutorial we will use a random forest classifier to implement this model \n",
    "\n",
    "## 2.2 Install the dependencies\n",
    "\n",
    "```pip install -r requirements.txt```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import mlflow.sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase 1. Train a model in an old school way"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# calculate an accuracy from the confusion matrix\n",
    "def get_model_accuracy(cf_matrix):\n",
    "    diagonal_sum = cf_matrix.trace()\n",
    "    sum_of_all_elements = cf_matrix.sum()\n",
    "    return diagonal_sum / sum_of_all_elements\n",
    "\n",
    "\n",
    "def train_model(data_url:str,n_estimator:int, max_depth:int, min_samples_split:int):\n",
    "    print(f\"data source: {data_url}\")\n",
    "    feature_data, label_data = prepare_data(data_url)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(feature_data, label_data, train_size=0.8, test_size=0.2,\n",
    "                                                        random_state=0)\n",
    "    # print(len(test_X))\n",
    "\n",
    "    # create a random forest classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth,\n",
    "                                    min_samples_split=min_samples_split,\n",
    "                                    n_jobs=2, random_state=0)\n",
    "    # train the model with training_data\n",
    "    rf_clf.fit(train_X, train_y)\n",
    "    # predict testing data\n",
    "    predicts_val = rf_clf.predict(test_X)\n",
    "\n",
    "    # Generate a cm\n",
    "    cm = confusion_matrix(test_y, predicts_val)\n",
    "    model_accuracy = get_model_accuracy(cm)\n",
    "    print(\"RandomForest model with hyper-parameters: (n_estimator=%f, max_depth=%f, min_samples_split=%f):\" % (n_estimator, max_depth, min_samples_split))\n",
    "    print(\"accuracy: %f\" % model_accuracy)\n",
    "\n",
    "\n",
    "def prepare_data(data_url):\n",
    "    # read data as df\n",
    "    try:\n",
    "        input_df = pd.read_csv(data_url, index_col=0)\n",
    "        input_df.head()\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Unable to read data from the giving path, check your data location. Error: %s\", e\n",
    "        )\n",
    "    # Prepare data for ml model\n",
    "    label = input_df.legendary\n",
    "    feature = input_df.drop(['legendary', 'generation', 'total'], axis=1).select_dtypes(exclude=['object'])\n",
    "    return feature, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data source: https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\n",
      "RandomForest model with hyper-parameters: (n_estimator=50.000000, max_depth=30.000000, min_samples_split=2.000000):\n",
      "accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(40)\n",
    "# set the training data path\n",
    "data_url = \"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "\n",
    "# set the hyper parameters\n",
    "n_estimator = 50\n",
    "max_depth = 30\n",
    "min_samples_split = 2\n",
    "\n",
    "# train the model\n",
    "train_model(data_url,n_estimator, max_depth, min_samples_split)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase 2. Train a model with model tracking tools (CI)\n",
    "\n",
    "In this tutorial, we use mlflow as our model tracking tools. Before you start the phase 2, you need to launch the mlflow service in datalab\n",
    "\n",
    "![mlflow_datalab.PNG](img/mlflow_datalab.PNG)\n",
    "\n",
    "Once you launched the mlflow service you need to create an experiment(project) if you don't have one. The name of the experiment is important, and we will see how to use it in the following code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model_with_mlflow_tracking(mlflow_experiment_name: str, mlflow_run_name: str, data_url: str, n_estimator: int, max_depth: int,\n",
    "                 min_samples_split: int):\n",
    "    # Step1: Prepare data\n",
    "    feature_data, label_data = prepare_data(data_url)\n",
    "    train_X, test_X, train_y, test_y = train_test_split(feature_data, label_data, train_size=0.8, test_size=0.2,\n",
    "                                                        random_state=0)\n",
    "    # set up mlflow context\n",
    "    mlflow.set_experiment(mlflow_experiment_name)\n",
    "    with mlflow.start_run(run_name=mlflow_run_name):\n",
    "        # create a random forest classifier\n",
    "        rf_clf = RandomForestClassifier(n_estimators=n_estimator, max_depth=max_depth,\n",
    "                                        min_samples_split=min_samples_split,\n",
    "                                        n_jobs=2, random_state=0)\n",
    "        # train the model with training_data\n",
    "        rf_clf.fit(train_X, train_y)\n",
    "        # predict testing data\n",
    "        predicts_val = rf_clf.predict(test_X)\n",
    "\n",
    "        # Generate a cm\n",
    "        cm = confusion_matrix(test_y, predicts_val)\n",
    "        model_accuracy = get_model_accuracy(cm)\n",
    "        print(\"RandomForest model with hyper-parameters: (n_estimator=%f, max_depth=%f, min_samples_split=%f):\" % (n_estimator, max_depth,\n",
    "                                                                                            min_samples_split))\n",
    "        print(\"accuracy: %f\" % model_accuracy)\n",
    "        # log the model hyper-parameters to the mlflow server\n",
    "        mlflow.log_param(\"data_url\", data_url)\n",
    "        mlflow.log_param(\"n_estimator\", n_estimator)\n",
    "        mlflow.log_param(\"max_depth\", max_depth)\n",
    "        mlflow.log_param(\"min_samples_split\", min_samples_split)\n",
    "\n",
    "        # log shap feature explanation extension. This will generate a graph of feature importance of the model\n",
    "        # mlflow.shap.log_explanation(rf_clf.predict, test_X.sample(70))\n",
    "\n",
    "        # log the model accuracy to the mlflow server\n",
    "        mlflow.log_metric(\"model_accuracy\", model_accuracy)\n",
    "\n",
    "        # log the model to the mlflow server\n",
    "        mlflow.sklearn.log_model(rf_clf, \"model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To make the model training more flexible, we also convert the above jupyter notebook to a python script. Fot the full code, please check [here](tutorials/pokemon/train_model.py)\n",
    "\n",
    "And we write a little bash script to run the python with specific env var\n",
    "\n",
    "```shell\n",
    "#! /bin/bash\n",
    "export MLFLOW_S3_ENDPOINT_URL='https://minio.lab.sspcloud.fr'\n",
    "export MLFLOW_TRACKING_URI='https://user-pengfei-866801.kub.sspcloud.fr/'\n",
    "export MLFLOW_EXPERIMENT_NAME=\"pokemon\"\n",
    "\n",
    "run_name=\"default\"\n",
    "data_url=\"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "\n",
    "# set the hyper parameters\n",
    "n_estimator=\"50\"\n",
    "max_depth=\"30\"\n",
    "min_samples_split=\"2\"\n",
    "\n",
    "root_path=\"/home/jovyan/work/MLOPS\"\n",
    "\n",
    "python ${root_path}/tutorials/pokemon/train_model.py ${MLFLOW_EXPERIMENT_NAME} ${run_name} ${data_url} ${n_estimator} ${max_depth} ${min_samples_split}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! sh tutorials/pokemon/bash_command/local_run.sh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "But we still need to set up the python `virtual environment` and git clone the code, can we do better?\n",
    "\n",
    "Yes, we can. Thanks to the mlflow, which offers a launching API which can build the virtual environment and get the code automatically.\n",
    "\n",
    "We only need to setup two config files\n",
    "- MLproject\n",
    "- conda.yaml\n",
    "\n",
    "Below is our `MLporject`:\n",
    "```yaml\n",
    "name: pokemon-legendary-estimator\n",
    "\n",
    "conda_env: conda.yaml\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      remote_server_uri: {type: str, default: http://pengfei.org:8000}\n",
    "      experiment_name: {type: str, default: test-1}\n",
    "      run_name: {type: str, default: default}\n",
    "      data_url: {type: str, default: https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv}\n",
    "      n_estimator: {type: int, default: 10}\n",
    "      max_depth: {type: int, default: 5}\n",
    "      min_samples_split: {type: int, default: 2}\n",
    "    command: \"python train_model.py {experiment_name} {run_name} {data_url} {n_estimator} {max_depth} {min_samples_split}\"\n",
    "```\n",
    "\n",
    "and conda.yaml\n",
    "\n",
    "```yaml\n",
    "name: pokemon-legendary-estimator\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - pip\n",
    "  - pip:\n",
    "    - scikit-learn==1.1.2\n",
    "    - mlflow>=1.28.0\n",
    "    - pandas>=1.2.2\n",
    "    - numpy>=1.20.1\n",
    "    - shap>=0.39.0\n",
    "    - matplotlib>=3.4.1\n",
    "    - boto3==1.17.19\n",
    "```\n",
    "\n",
    "And now we can train a model without installing anything with below bash script\n",
    "\n",
    "```shell\n",
    "#! /bin/bash\n",
    "export MLFLOW_S3_ENDPOINT_URL='https://minio.lab.sspcloud.fr'\n",
    "export MLFLOW_TRACKING_URI='https://user-pengfei-866801.kub.sspcloud.fr/'\n",
    "export MLFLOW_EXPERIMENT_NAME=\"pokemon\"\n",
    "\n",
    "run_name=\"default\"\n",
    "data_url=\"https://minio.lab.sspcloud.fr/pengfei/sspcloud-demo/pokemon-cleaned.csv\"\n",
    "\n",
    "# set the hyper parameters\n",
    "n_estimator=\"50\"\n",
    "max_depth=\"30\"\n",
    "min_samples_split=\"2\"\n",
    "\n",
    "mlflow run https://github.com/pengfei99/mlflow-pokemon-example.git -P remote_server_uri=$MLFLOW_TRACKING_URI -P experiment_name=$MLFLOW_EXPERIMENT_NAME \\\n",
    "-P data_url=https://minio.lab.sspcloud.fr/pengfei/mlflow-demo/pokemon-partial.csv \\\n",
    "-P n_estimator=50 -P max_depth=30 -P min_samples_split=2\n",
    "```\n",
    "\n",
    "Now, you can see we let the mlflow download the code and create virtual environment for us"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! sh tutorials/pokemon/bash_command/remote_run.sh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase 3 Train many models in parallel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}